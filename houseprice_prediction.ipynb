importing the dependecies
1. Numpy is used for mathematical calculations
2. Pandas for working with datasets.
3. mathplot and seaborn to plot and visualsie graphs.
4. Sklearn is data analysing library in short it is used for all processings .
5. Xgboost is for regression

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.datasets
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn import metrics


importing California dataset (boston wala nahi chal rha)

house_price_dataset = sklearn.datasets.fetch_california_housing()
print (house_price_dataset)

# loading data into pandas for a structured data
house_price_dataframe=pd.DataFrame(house_price_dataset.data, columns=house_price_dataset.feature_names)

house_price_dataframe.head()

# adding the target coloum to dataframe for house prices
house_price_dataframe['price']= house_price_dataset.target

house_price_dataframe.head()


#checking the number of rows and coloums in the dataframe
house_price_dataframe.shape

# means we have 20640 rows and 9 coloums.

# checking if we have any missing vlaues in the dataframe
house_price_dataframe.isnull().sum()

#hence no missing values in the dataset...we are good to go now

#stats of the dataset (mean, standard deviation,median etc)
house_price_dataframe.describe()

Understanding the co-relation between features in dataset
1. Postive Co-relation means if one variable increases then the other will increase as well 
2. Negative Co-relation means if one variable increase then the other will decrease as well

#analysing the co-relations
corelation=house_price_dataframe.corr()


#constrcuing  a heatmap to understand the co-relations
plt.figure(figsize=(10,10))
sns.heatmap(corelation ,cbar=True,square=True,fmt='.1f',annot=True,annot_kws={'size':8},cmap="Blues")

#now spilting the data for better use
x= house_price_dataframe.drop (['price'],axis=1)
# we are droping the coloumn price from the dataframe and axis =1 signifies that its a coloum
y=house_price_dataframe['price']

# what we have done is we made a copy of dataframe in which there is no price coloum and stored it in X and we used Y to store the price
# i.e we splited the price from the dataframe 

print (x,y)


#spliting data into training data and test  data
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state= 2)

#test_size = 0.2 means we have 20% test data and remaining as training data


# Time to train the model now using XG boost regressor 


#Loading the model now 

model = XGBRegressor()

#training the data now 
model.fit(x_train,y_train)






# Evaluation for accuracy 


#accuracy for predication for training data
training_data_prediction=model.predict(x_train)


# R squared error and mean absolute erro
score_1=metrics.r2_score(y_train,training_data_prediction)

# mean absolute error
score_2=metrics.mean_absolute_error(y_train,training_data_prediction)

print (score_1,score_2)


# now we will test our model with testing data
test_data_prediction=model.predict(x_test)
score_3=metrics.r2_score(y_test,test_data_prediction)
score_4=metrics.mean_absolute_error(y_test,test_data_prediction)
print (score_3,score_4)


# visualising the prices and predicated prices
plt.scatter(y_train,training_data_prediction)
plt.xlabel("actual prices")
plt.ylabel("predication")
plt.title("actual v/s predictions")
plt.show()

# the graph should have datapoints as close as possible

